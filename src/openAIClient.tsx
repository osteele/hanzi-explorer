import { camelToSnakeCase } from "./utils";

export type CompletionParameters = {
  /**
   * The name of the model to use for generating completions. If not
   * provided, uses the default model.
   */
  model: string;

  /**
   * The prompt for which completions are required.
   */
  prompt: string | string[];

  /**
   * Controls the "creativity" of the generated completions. Must be between 0
   * and 1. Lower values result in more conservative completions, while higher
   * values result in more creative completions.
   */
  temperature?: number;

  /**
   * The maximum number of tokens (words or word pieces) in the generated
   * completions.
   */
  maxTokens?: number;

  /**
   * The number of completions to generate.
   */
  numResults?: number;

  /**
   * Controls how much the generator leverages existing context when producing
   * a new completion. Must be between 0 and 1. Higher values means that the
   * generator will be more "responsive" to the input prompt.
   */
  presencePenalty?: number;

  /**
   * Controls how much the generator penalizes repeated phrase structures in
   * the completions it generates. Higher values mean that the generator is less
   * likely to produce completions with repeated phrases.
   */
  frequencyPenalty?: number;

  /**
   * A string or an array of strings representing tokens at which to stop
   * generation of completions.
   */
  stop?: string | string[];
};

export type CompletionParameterSetters = {
  setPrompt: (prompt: string) => void;
  setModel: (model: string) => void;
  setFrequencyPenalty: (frequencyPenalty: number) => void;
  setMaxTokens: (maxTokens: number) => void;
  setNumResults: (numResults: number) => void;
  setPresencePenalty: (presencePenalty: number) => void;
  setTemperature: (temperature: number) => void;
  // [Property in keyof CompletionParameters as `set${Capitalize<string & Property>}`]: (Type[Property]) => void;
};

function modelIsChatModel(model: string): boolean {
  return model.startsWith("gpt-");
}

export type Completions = {
  prompt: string;
  completion: string;
  isChat: boolean;
}[];

export type GetCompletionsParams = {
  apiKey: string;
  onProgress?: (choices: Completions) => void;
} & CompletionParameters;

/**
 * This function retrieves completions for a given prompt from OpenAI's API.
 *
 * @param props - Object containing the parameters required to retrieve
 * completions from OpenAI's API.
 *
 * @returns An object containing a Promise that resolves to an array of choices
 * generated by the API.
 */
export async function getCompletions(
  props: GetCompletionsParams & { signal: AbortSignal }
): Promise<{ data: { choices: Completions } }> {
  const { apiKey, onProgress, signal, ...modelParameters } = props;

  // If the prompt is an array, apply streamingCompletion to each prompt,
  // combine the results into one array of choices, and return a Promise
  // for this array
  const { numResults, prompt } = { numResults: 1, ...modelParameters };
  if (Array.isArray(prompt)) {
    const choices = Array.from({ length: numResults * prompt.length }, () => ({
      prompt: "",
      completion: "",
      isChat: false,
    }));
    // Call getCompletions recursively to retrieve completions for each
    // individual prompt in the array
    await Promise.all(
      prompt.map((prompt, i) =>
        getCompletions({
          ...modelParameters,
          apiKey,
          prompt,
          signal,
          onProgress: (choicesSlice) => {
            choices.splice(i * numResults, numResults, ...choicesSlice);
            onProgress?.(choices);
          },
        })
      )
    );
    return {
      data: {
        choices,
      },
    };
  }

  // Decide which endpoint to use depending on whether the model is a chat model
  // or not
  const isChatModel = modelIsChatModel(modelParameters.model);
  const endpoint = isChatModel ? "chat/completions" : "completions";

  // Prepare model parameters as required by the endpoint (i.e., add messages
  // object for chat models)
  const preparedModelParameters = isChatModel
    ? {
        messages: [
          {
            role: "user",
            content: prompt,
          },
        ],
        ...modelParameters,
        prompt: undefined,
      }
    : modelParameters;

  // If debug mode is enabled, log the request prompt
  if (process.env.DEBUG) {
    console.debug("requesting", prompt);
  }

  // Send the request to OpenAI's API with required headers and data
  const response = await fetch(`https://api.openai.com/v1/${endpoint}`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      stream: true,
      ...renameModelParameterNames(preparedModelParameters),
    }),
    signal,
  });

  // Handle errors by parsing the error message provided by the API
  if (response.status >= 400) {
    const text = await response.text();
    let e = new Error(text);
    try {
      const json = JSON.parse(text);
      e = json.error;
    } catch {}
    throw e;
  }

  // Read response as a readable stream of newline-separated values, and process
  // incoming data
  const reader = response.body
    ?.pipeThrough(new TextDecoderStream())
    .getReader();
  if (!reader) {
    throw new Error("Internal error: failed to create TextDecoderStream");
  }

  // Initialize choices array
  const choices = Array.from({ length: numResults }, () => ({
    prompt,
    completion: "",
    isChat: isChatModel,
  }));

  // Process incoming data line-by-line until the "DONE" message is received
  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    if (
      value.split("\n").some((line) => {
        if (line === "data: [DONE]") {
          // Stop processing when the "DONE" message is received
          return true;
        } else if (line.startsWith("data: ")) {
          // Process each incoming message
          const data = JSON.parse(line.slice(6));
          for (const choice of data.choices) {
            // Concatenate completion text to previous completions
            const index = choice.index;
            choices[index].completion +=
              choice.text ?? choice.delta.content ?? "";
          }
          onProgress?.(choices);
        }
        return false;
      })
    ) {
      break;
    }
  }

  reader.releaseLock(); // not in a `finally`; but isn't strictly necessary anyway
  return { data: { choices } };
}

/**
 * Renames keys in an object according to specified rules.
 *
 * @param parameters - The object whose keys should be renamed.
 * @returns A new object with renamed keys based on the original keys and values
 *          from the input object.
 *
 * @example
 * const parameters = { maxResults: 10, pageToken: "abc123", numResults: 5 };
 * const renamedParams = renameModelParameterNames(parameters);
 * // renamedParams == { max_results: 10, page_token: "abc123", n: 5 }
 */
function renameModelParameterNames(parameters: { [key: string]: any }): {
  [key: string]: any;
} {
  const result: { [key: string]: any } = {};

  for (const [key, value] of Object.entries(parameters)) {
    if (key === "numResults") {
      result["n"] = value;
    } else {
      const snakeCase = camelToSnakeCase(key);
      result[snakeCase] = value;
    }
  }

  return result;
}
